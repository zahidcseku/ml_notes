{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591c7f34",
   "metadata": {},
   "source": [
    "# Deep neural network notation\n",
    "\n",
    "```{contents}\n",
    "```\n",
    "\n",
    "A deep neural network (DNN) consists of multiple layers of computational units, called neurons. Each layer is arranged sequentially, and the units in each layer are arranged vertically. Each neuron (or unit) takes some input and produces an output. Consequently, a layer takes some inputs and produces some outputs using simple computations. When we say each layer takes inputs, we mean that each neuron in the layers takes inputs. In this section, I will discuss various notations used in deep neural networks. Most of the notations are adapted from Andrew Ng’s deep learning specialisation course. The notations involve multiple dimensions that can sometimes be confusing (actually, all the time when you start learning from different sources). When I started learning DNN, I got familiar with one set of notations from Andrew’s course and started representing other materials using the same notations (it was not a trivial task at the beginning).\n",
    "\n",
    "## Inputs and outputs of a single neuron\n",
    "Each neuron takes arbitrary $n_X$ dimensional input $\\mathbf X$ and produces a single output $a$. \n",
    "\n",
    "```{figure} single-neuron.png\n",
    "---\n",
    "name: single_neuron\n",
    "width: 200px\n",
    "align: center\n",
    "---\n",
    "A single neural processing unit which takes $n_X$ dimensional inputs and generates a scalar output $a$.\n",
    "```\n",
    "\n",
    "The neuron's output is calculated in two steps, as indicated by a circular + symbol and a rectangular $\\sigma$ symbol. A weighted sum is first computed as follows:\n",
    "\n",
    "$$z = x_0*w_0 + x_1*w_1 + \\cdots + x_{n_X}*w_{n_X} + b = \\sum_{i=0}^{n_X}x_i*w_i +b$$\n",
    "\n",
    "And then an activation function $\\sigma$ is applied to $z$ to compute the output (called activation) of the unit as follows:\n",
    "\n",
    "$$a = \\sigma\\left(z\\right)$$\n",
    "\n",
    "If we express the input and the weights as row vectors $\\mathbf W = [w_0, w_1, \\cdots, w_{n_X}]$ and $\\mathbf X =[x_1, x_2,\\dots,x_{n_X}]$, we can achieve the same result using a vector operation:\n",
    "\n",
    "$$a = \\sigma(z) = \\sigma(\\mathbf W \\cdot \\mathbf X^\\top + b)$$\n",
    "\n",
    "\n",
    "**Shapes**: both the $\\mathbf W$ and $\\mathbf X$ are row vectors of shape $(1, n_X)$ and both $z$ and $a$ are scalars.\n",
    "\n",
    "```{list-table} Notations - single neuron\n",
    ":header-rows: 1\n",
    ":name: notation-single-neuron\n",
    "\n",
    "* - Symbol\n",
    "  - Shape\t\n",
    "  - Definition\n",
    "* - $\\mathbf X$\t\n",
    "  - $(1, n_X)$\t\n",
    "  - The input row vector. \n",
    "* - $n_X$\t\n",
    "  - 1\t\n",
    "  - The dimension of the inputs.\n",
    "* - $x_j$\t\n",
    "  - $1$\t\n",
    "  - The value of the $j$-th feature of an input object.\n",
    "* - $\\mathbf W$\t\n",
    "  - $(1, n_X)$\t\n",
    "  - Weight vector of the neuron.\n",
    "* - $b$\t\n",
    "  - 1\t\n",
    "  - Bias parameter of the neuron.\n",
    "* - $z$\t\n",
    "  - 1\t\n",
    "  - The intermediate output of the neuron before applying the activation function.\n",
    "* - $a$\n",
    "  - 1\t\n",
    "  - The output (or activation) of the neuron after applying the activation function $\\sigma$ to $z$.\n",
    "* - $\\mathbf X^\\top$\n",
    "  - $(n_X, 1)$\t\n",
    "  - $\\mathbf X$ transposed.\n",
    "```\n",
    "\n",
    "\n",
    "## Inputs and outputs of a single layer\n",
    "A layer in a neural network is created by stacking multiple neural units. Each unit takes the same number of real numbers ($n_X$) and produces a single output. The number of outputs of a layer is determined by the number of units in the layer. Representing a layer by $l$, the number of units in the layer is expressed as $n^{[l]}$.\n",
    "\n",
    "```{figure} single_layer.png\n",
    "---\n",
    "name: single_layer\n",
    "width: 200px\n",
    "align: center\n",
    "---\n",
    "A single layer processing unit that takes $n_X$ dimensional inputs and produces $n^{[l]}$ dimensional outputs. Here, $n^{[l]}$ is the number of processing units in layer $l$.\n",
    "```\n",
    "\n",
    "Now that we have multiple units, we can't express the weights as a vector anymore. The weights of each individual unit in the layer are still a vector (shape $(1, n_X)$), and we need a matrix notation (by stacking the vectors) to express the weights of layer $l$. For $n^{[l]}$ units, we will have $n^{[l]}$ biases expressed as $\\mathbf b^{[l]}$, which is a vector of shape $(n^{[l]}, 1)$. We have multiple outputs from a layer, hence both $a$ and $z$ are now vectors of shape $(n^{[l]}, 1)$ and represented by: $\\mathbf z^{[l]}$ and $\\mathbf a^{[l]}$ respectively. Layer $l$ performs the following operations:\n",
    "\n",
    "$$a^{[l]}_0 = \\sigma^{[l]} (z^{[l]}_0) = \\sigma^{[l]} \\left(w^{[l]}_{00}x_0 + w^{[l]}_{01}x_1+\\dots+w^{[l]}_{0n_X}x_{n_X}+b^{[l]}_0\\right) = \\sigma^{[l]} \\left(\\sum_i^{n_X} w^{[l]}_{0i}x^i + b^{[l]}_0\\right)$$\n",
    "\n",
    "$$a^{[l]}_1 = \\sigma^{[l]} (z^{[l]}_1) = \\sigma^{[l]} \\left(w^{[l]}_{10}x_0 + w^{[l]}_{11}x_1+\\dots+w^{[l]}_{1n_X}x_{n_X}+b^{[l]}_1\\right)=\\sigma^{[l]}\\left(\\sum_{i}^{n_X} w^{[l]}_{1i}x_i + b^{[l]}_1\\right)$$\n",
    "\n",
    "$$\\dots$$\n",
    "\n",
    "$$a^{[l]}_{n^{[l]}} = \\sigma^{[l]} (z^{[l]}_{n^{[l]}}) = \\sigma^{[l]} \\left(w^{[l]}_{n^{[l]}0}x_0 + w^{[l]}_{n^{[l]}1}x_1+\\dots+w^{[l]}_{n^{[l]}n_X}x_{n_X}+b^{[l]}_{n^{[l]}}\\right)=\\sigma^{[l]}\\left(\\sum_{i}^{n_X} w^{[l]}_{n^{[l]}i}x_i + b^{[l]}_{n^{[l]}}\\right)$$\n",
    "\n",
    "We can implement the above computations in python using only multiplications and additions.\n",
    "\n",
    "```python\n",
    "output = []\n",
    "for i in range(nl):\n",
    "\tz = 0\n",
    "\tfor j in range(n):\n",
    "\t\tz += W[i][j] * x[j]\n",
    "\tz += b[i]\n",
    "\ta = sigma(z)\n",
    "\t\n",
    "\toutput.append(a)\n",
    "```\n",
    "\n",
    "The equations and computations can be simplified using matrix operations. The weights of the matrix can be represented by:\n",
    "\n",
    "$$\n",
    "\\mathbf W^{[l]} = \\begin{bmatrix} w^{[l]}_{00} & w^{[l]}_{01} & \\cdots & w^{[l]}_{0n_X}\\\\ w^{[l]}_{10} & w^{[l]}_{11} & \\cdots & w^{[l]}_{1n_X}\\\\\\cdots&\\cdots&\\cdots&\\cdots\\\\w^{[l]}_{n^{[l]}0} & w^{[l]}_{n^{[l]}1} & \\cdots & w^{[l]}_{n^{[l]}n_X}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The dimension of $\\mathbf W^{[l]}$ depends on the input dimension $n_X$ and the number of units (which is same as the output dimension of the layer) $n^{[l]}$ i.e., $(n^{[l]}, n_X)$. Using the matrix notations, the computations are:\n",
    "\n",
    "$$\\mathbf a^{[l]} = \\sigma^{[l]} (\\mathbf z^{[l]}) = \\sigma^{[l]}\\left(\\mathbf W^{[l]} \\cdot \\mathbf X^\\top + \\mathbf b^{[l]}\\right)$$\n",
    "\n",
    "In python implementation,\n",
    "\n",
    "```python\n",
    "z = np.matmul(W, x.T) + b\n",
    "a = sigma(z)\n",
    "```\n",
    "\n",
    "\n",
    "```{list-table} Notations - single layer of neurons\n",
    ":header-rows: 1\n",
    ":name: notation-single-layer\n",
    "* - Symbol\n",
    "  - Shape\n",
    "  - Definition\n",
    "* - $l$\n",
    "  - $1$ \n",
    "  - The layer id.  \n",
    "* - $n^{[l]}$ \n",
    "  - $1$\n",
    "  - Number of units in layer $l$.\n",
    "* - $\\mathbf b^{[l]}$\n",
    "  - $(n^{[l]}, 1)$ \n",
    "  - The bias vector corresponding to layer $l$. \n",
    "* - $b^{[l]}_i$ \n",
    "  - $1$\n",
    "  - Bias of the $i$-th unit in layer $l$.\n",
    "* - $\\mathbf W^{[l]}$\n",
    "  - $(n^{[l]}, n_X)$\n",
    "  - The weights corresponding to the layer $l$.\n",
    "* - $w^{[l]}_{ij}$\n",
    "  - $1$\n",
    "  - The weight corresponding to unit $i$ and feature $j$ in layer $l$.\n",
    "* - $\\mathbf a^{[l]}$\n",
    "  - $(n^{[l]}, 1)$\n",
    "  - Activations (outputs) of layer $l$.\n",
    "* - $\\mathbf z^{[l]}$\n",
    "  - $(n^{[l]}, 1)$\n",
    "  - The intermediate outputs of the neurons given the input  $\\mathbf X$ before applying the activation function.\n",
    "* - $a_i^{[l]}, z_i^{[l]}$\n",
    "  - $1$\n",
    "  - Elements of $\\mathbf a^{[l]}$ and $\\mathbf z^{[l]}$.\n",
    "* - $\\sigma^{[l]}$\n",
    "  - $~~$\n",
    "  - Activation function corresponding to layer $l$.\n",
    "```\n",
    "\n",
    "\n",
    "## Inputs and outputs of a multiple layers\n",
    "\n",
    "A single neuron unit or a single layer network is never used for solving practical machine learning problems. Building upon the discussion of a single neuron unit and a single layer of neurons, we now discuss the practical scenario where multiple layers of neurons are arranged sequentially as shown in the figure below. The input $\\mathbf X \\in \\mathbb R^{n_X}$ is fed to the model at layer $l=1$ which processes the input and produces an output vector $\\mathbf a^{[1]} \\in \\mathbb R^{n^{[1]}}$. The layer $2$ receives inputs $\\mathbf a^{n^{[1]}} \\in \\mathbb R^{n^{[1]}}$ from layer $1$ and produces outputs $\\mathbf a^{[2]} \\in \\mathbb R^{n^{[2]}}$ and so on. Remember that each neuron receives the same dimensional inputs and the output dimension of a layer is equal to the number of units. Each layer (except the first layer) receives input from the previous layer. The number of units in a layer is a design choice (except the last layer $L$) determined through hyperparameter search. The last layer from which we obtain the outputs of the model is called the output layer. The input dimension of the first layer and the number of units of layer $L$ (or the output dimension of layer $L$) depends on the task. For example, for the digit classification of $28\\times 28$ images of digits, the input dimension $n_X=28*28=784$ and the output dimension or the number of units in layer $L$ is $10$.\n",
    "\n",
    "\n",
    "```{figure} multi_layer_model.png\n",
    "---\n",
    "name: multi_layer_model\n",
    "width: 500px\n",
    "align: center\n",
    "---\n",
    "A multilayer deep neural network (or a multilayer perceptron) model. Each layer $l$ consists of $n^{[l]}$ nuerons. \n",
    "```\n",
    "\n",
    "All the symbols used in the above [figure](multi_layer_model) are covered in the previous section: Inputs and Outputs of a Single Layer. Often, we use $\\mathbf {\\hat y}$ to represent the output of the model. So, we can say $\\mathbf{\\hat y}=\\mathbf a^{[L]}$. In addition to our previous symbols, we introduce the following symbol for multilayer models.\n",
    "\n",
    "\n",
    "```{list-table} Notations - multiple layers of neurons\n",
    ":header-rows: 1\n",
    ":name: notation-multiple-layers\n",
    "* - Symbol\n",
    "  - Shape\n",
    "  - Definition\n",
    "* - $L$\n",
    "  - 1\n",
    "  - The number of layers.\n",
    "* - $1,2,\\dots,l,\\dots,L$\n",
    "  - $1$\n",
    "  - Layer id.\n",
    "* - $\\mathbf{\\hat y} = \\mathbf a^{[L]}$\n",
    "  - $(a^{[L]}, 1)$\n",
    "  - Output of the model.\n",
    "* - $\\mathbf X = \\mathbf a^{[0]}$\n",
    "  - $(1, n_X)$\n",
    "  - Inputs in terms of activation.\n",
    "```\n",
    "\n",
    "Using the activation notations, $\\mathbf a^{[0]}=\\mathbf X^\\top$ and $\\mathbf a^{[L]}=\\mathbf{\\hat y}$ we can draw as general eqaution ov the network as:\n",
    "\n",
    "$$\\mathbf z^{[l]} = W^{[l]}\\cdot \\mathbf a^{[l-1]} + \\mathbf b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = \\sigma^{[l]}\\left (\\mathbf z^{[l]}\\right)$$\n",
    "\n",
    "for $l=1,2,\\dots,L$\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In the above discussion, we considered our input $\\mathbf X\\in\\mathbb R^{n_X}$ to be a $n_X$ dimensional row vector. However, in real applications, the input is a $2$ or $3$ dimensional tensor (we consider 2d cases only for now). For supervised learning, we have many instances of $\\mathbf X$ and their associated labels $\\mathbf y$. For unsupervised learning tasks, we do not have $\\mathbf y$. We represent the dataset as $\\mathcal D=(\\mathbf X, \\mathbf y)$. The shape of $\\mathbf X$ is given by $(n_{\\mathcal D}, n_X)$ and the shape of $\\mathbf y$ is $(n_\\mathcal D, n_y)$. To refer to any instance of $\\mathcal D$, we use the superscript notation i.e., $\\mathbf X^{(i)}$ refers to the $i$-th instance of $\\mathbf X$ and $\\mathbf y^{(i)}$ is the corresponding label. The superscript notation can also be used to refer to any layer computation, e.g., $\\mathbf z^{(i)[l]}$ refers to the output of layer $l$ corresponding to the $i$ instance in the dataset.\n",
    "\n",
    "```{note} we used the same $\\mathbf X$ to refer to the input as a vector above for simplicity. We should have used $\\mathbf X^{(i)}$. With this dataset notation in the above computations, all output shapes will change as follows: $\\mathbf z^{[l]}$ and $\\mathbf a^{[l]}$ now include $n_{\\mathcal D}$ instances corresponding to the number of samples in the dataset.\n",
    "```\n",
    "\n",
    "```{note} For efficiency, the dataset is divided into batches of inputs and the network processes a batch at a time rather than the entire batch at a time.\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "The [iris dataset:](https://archive.ics.uci.edu/dataset/53/iris) includes 150 instances of iris flower. Each isntance is described by four feautures {sepal length, sepal width, petal length, petal width}. In our notation,  the iris dataset can be described as follows:\n",
    "\n",
    "- $n_{\\mathcal D} =150$\n",
    "- $n_X=4$\n",
    "- $n_y=1$\n",
    "- $\\mathbf X^{(2)}= [4.9, 3.0, 1.4, 0.2]$\n",
    "- $\\mathbf y^{(2)} = [C1]$\n",
    "- $x^{(150)}_3=5.1$\n",
    "\n",
    "```{figure} dataset.png\n",
    "---\n",
    "name: dataset\n",
    "width: 250px\n",
    "align: center\n",
    "---\n",
    "The iris dataset.\n",
    "```\n",
    "\n",
    "## Code example\n",
    "In this code example, we will generate 10 random instances of 5 dimensional inputs i.e., $n_\\mathcal D=10$, $n_X=5$. Let's consider the network consists of three layers ($L=3$) with $n^{[1]} = 8$, $n^{[2]} = 12$, and $n^{[3]} = 2$. The dimensions of our weight metrices will be $\\mathbf W^{[1]}$, $\\mathbf W^{[2]}$ and $\\mathbf W^{[3]}$ are $(8, 5)$, $(12, 8)$ and $(2, 12)$. The shapes of the biases $\\mathbf b^{[1]}$, $\\mathbf b^{[2]}$ and $\\mathbf b^{[3]}$ are $(8, 1)$, $(12, 1)$ and $(2, 1)$. We will generate the parameters randomly and using the equations discussed above we will compute $\\mathbf a^{[l]}, l = 1,2,3$. We assume $\\mathbf a^{[l]} = \\mathbf z^{[l]} | l =1,2,3$ i.e., we do not apply the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a9ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a1: (8, 10)\n",
      "Shape of a2: (12, 10)\n",
      "Shape of a3: (2, 10)\n",
      "Outputs of the network: [[48.97514768 40.78006352 24.55718999 30.20303075 32.66306258 48.1979801\n",
      "  49.94261146 28.00091955 32.59358435 41.0307172 ]\n",
      " [59.56719231 49.76893816 29.69656509 36.26551186 39.84596245 58.73617283\n",
      "  61.01766603 33.70259162 39.87323399 49.74898875]]\n"
     ]
    }
   ],
   "source": [
    "# set up the variables\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nd, nx, nl = 10, 5, 3\n",
    "n1, n2, n3 = 8, 12, 2\n",
    "\n",
    "# input and parameters initializations\n",
    "x = np.random.rand(nd, nx)\n",
    "W1 = np.random.rand(n1, nx)\n",
    "b1 = np.random.rand(n1, 1)\n",
    "W2 = np.random.rand(n2, n1)\n",
    "b2 = np.random.rand(n2, 1)\n",
    "W3 = np.random.rand(n3, n2)\n",
    "b3 = np.random.rand(n3, 1)\n",
    "\n",
    "# layer wise computations\n",
    "a0 = x.T\n",
    "a1 = np.matmul(W1, a0) + b1\n",
    "print(f\"Shape of a1: {a1.shape}\")\n",
    "\n",
    "a2 = np.matmul(W2, a1) + b2\n",
    "print(f\"Shape of a2: {a2.shape}\")\n",
    "\n",
    "a3 = np.matmul(W3, a2) + b3\n",
    "print(f\"Shape of a3: {a3.shape}\")\n",
    "\n",
    "print(f\"Outputs of the network: {a3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa7456",
   "metadata": {},
   "source": [
    "Let's, verify the results using pytorch. We will initialize the pytorch parameters and inputs using the same random tensors as our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03783879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a1: torch.Size([10, 8])\n",
      "Shape of a1: torch.Size([10, 12])\n",
      "Shape of a3: torch.Size([10, 2])\n",
      "tensor([[48.9751, 59.5672],\n",
      "        [40.7801, 49.7689],\n",
      "        [24.5572, 29.6966],\n",
      "        [30.2030, 36.2655],\n",
      "        [32.6631, 39.8460],\n",
      "        [48.1980, 58.7362],\n",
      "        [49.9426, 61.0177],\n",
      "        [28.0009, 33.7026],\n",
      "        [32.5936, 39.8732],\n",
      "        [41.0307, 49.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "l1 = nn.Linear(nx, n1)\n",
    "l2 = nn.Linear(n1, n2)\n",
    "l3 = nn.Linear(n2, n3)\n",
    "\n",
    "tensorX = torch.from_numpy(x)\n",
    "\n",
    "# initialize same weights\n",
    "l1.weight.data = torch.from_numpy(W1)\n",
    "l1.bias.data = torch.from_numpy(b1.squeeze())\n",
    "\n",
    "l2.weight.data = torch.from_numpy(W2)\n",
    "l2.bias.data = torch.from_numpy(b2.squeeze())\n",
    "\n",
    "l3.weight.data = torch.from_numpy(W3)\n",
    "l3.bias.data = torch.from_numpy(b3.squeeze())\n",
    "\n",
    "a1 = l1(tensorX)\n",
    "print(f\"Shape of a1: {a1.shape}\")\n",
    "\n",
    "a2 = l2(a1)\n",
    "print(f\"Shape of a1: {a2.shape}\")\n",
    "\n",
    "a3 = l3(a2)\n",
    "print(f\"Shape of a3: {a3.shape}\")\n",
    "\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d9765",
   "metadata": {},
   "source": [
    "```{note} \n",
    "pytorch uses broadcasting in computations. Here, we used `squeeze()` to match the shape of the bias parameter with pytorch internals. \n",
    "```\n",
    "\n",
    "We see that outputs of both implementations are identical. That's what we expect!\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "An entire multilayer neural network can be expressed as a set of parameters - the weights and biases.\n",
    "\n",
    "$\\mathbf{\\hat y} = f_\\Theta(\\mathbf X)$ where $\\Theta = \\{\\mathbf W^{[l]}, \\mathbf b^{[l]}~|~l = 1,2, \\dots, L\\}$.\n",
    "\n",
    "Let's consider the number of units of a 3 layer network to be $n^{[1]}=8, n^{[2]}=16, n^{[3]}=3$ and the input dimension $(n_\\mathcal D, n_X) = (500, 4)$. In layer 1, we have 8 units i.e., the output dimensions of layer 1 (also the input dimension of layer 2) is 8, $\\mathbf W^{[1]}$ will have 8 rows (weights of each unit is a row in the weight matrix) and each unit of layer 1 will process 4 dimensional inputs (as $n_X=4$) that means each unit will require 4 weights i.e, $\\mathbf W^{[1]}$ has 4 columns. The shape of $\\mathbf W^{[1]}$ is $(8, 4)$. In our notation, the input $\\mathbf X^\\top = \\mathbf a^{[0]}$, the output of layer 1 is $\\mathbf a^{[1]}$ which is $8$ dimensional that implies each unit of layer 2 will process $8$ dimensional inputs i.e., $\\mathbf W^{[2]}$ will have $8$ columns and with $n^{[2]}=16$ units the number of rows in $\\mathbf W^{[2]}$ is $16$. Similarly, the shape of $\\mathbf W^{[3]}$ is $(3, 16)$.\n",
    "\n",
    "The dimensions of $\\mathbf W$ and $\\mathbf b$ are:\n",
    "\n",
    "- For, $l=1$, shape of $\\mathbf W^{[1]}=(8,4)$ and shape of $\\mathbf b^{[1]}=(8,1)$.\n",
    "- For, $l=2$, shape of $\\mathbf W^{[2]}=(16,8)$ and shape of $\\mathbf b^{[2]}=(16,1)$.\n",
    "- For, $l=3$, shape of $\\mathbf W^{[3]}=(3,16)$ and shape of $\\mathbf b^{[3]}=(3,1)$.\n",
    "\n",
    "Computations in the network are:\n",
    "\n",
    "$$\\mathbf a^{[1]} = \\sigma^{[1]}\\left(W^{[1]}\\cdot \\mathbf a^{[0]} + \\mathbf b^{[1]}\\right)$$\n",
    "\n",
    "$$\\mathbf a^{[2]} = \\sigma^{[2]}\\left(W^{[2]}\\cdot \\mathbf a^{[1]} + \\mathbf b^{[2]}\\right)$$\n",
    "\n",
    "$$\\mathbf a^{[3]} = \\sigma^{[3]}\\left(W^{[3]}\\cdot \\mathbf a^{[2]} + \\mathbf b^{[3]}\\right)=\\mathbf{\\hat y}$$\n",
    "\n",
    "The shapes of $a^{[1]}, a^{[2]}$ and $a^{[3]}$  are $(8, 500), (16, 500)$ and $(3, 500)$. \n",
    "\n",
    "\n",
    "## Future plan\n",
    "\n",
    "In future, I will discuss the forward and backward propagation through a neural network using a complete example. Stay tuned:\n",
    "\n",
    "<style>\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview {\n",
    "display: flex !important;\n",
    "flex-direction: column !important;\n",
    "justify-content: center !important;\n",
    "margin-top: 30px !important;\n",
    "padding: clamp(17px, 5%, 40px) clamp(17px, 7%, 50px) !important;\n",
    "max-width: none !important;\n",
    "border-radius: 6px !important;\n",
    "box-shadow: 0 5px 25px rgba(34, 60, 47, 0.25) !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview,\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview *{\n",
    "box-sizing: border-box !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-heading {\n",
    "width: 100% !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-heading h5{\n",
    "margin-top: 0 !important;\n",
    "margin-bottom: 0 !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field {\n",
    "margin-top: 20px !important;\n",
    "width: 100% !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input {\n",
    "width: 100% !important;\n",
    "height: 40px !important;\n",
    "border-radius: 6px !important;\n",
    "border: 2px solid #e9e8e8 !important;\n",
    "background-color: #fff;\n",
    "outline: none !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input {\n",
    "color: #000000 !important;\n",
    "font-family: \"Montserrat\" !important;\n",
    "font-size: 14px;\n",
    "font-weight: 400;\n",
    "line-height: 20px;\n",
    "text-align: center;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input::placeholder {\n",
    "color: #000000 !important;\n",
    "opacity: 1 !important;\n",
    "}\n",
    "\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input:-ms-input-placeholder {\n",
    "color: #000000 !important;\n",
    "}\n",
    "\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input::-ms-input-placeholder {\n",
    "color: #000000 !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-submit-button {\n",
    "margin-top: 10px !important;\n",
    "width: 100% !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-submit-button button {\n",
    "width: 100% !important;\n",
    "height: 40px !important;\n",
    "border: 0 !important;\n",
    "border-radius: 6px !important;\n",
    "line-height: 0px !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-submit-button button:hover {\n",
    "cursor: pointer !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .powered-by-line {\n",
    "color: #231f20 !important;\n",
    "font-family: \"Montserrat\" !important;\n",
    "font-size: 13px !important;\n",
    "font-weight: 400 !important;\n",
    "line-height: 25px !important;\n",
    "text-align: center !important;\n",
    "text-decoration: none !important;\n",
    "display: flex !important;\n",
    "width: 100% !important;\n",
    "justify-content: center !important;\n",
    "align-items: center !important;\n",
    "margin-top: 10px !important;\n",
    "}\n",
    ".followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .powered-by-line img {\n",
    "margin-left: 10px !important;\n",
    "height: 1.13em !important;\n",
    "max-height: 1.13em !important;\n",
    "}\n",
    "\n",
    "</style>\n",
    "<div class=\"followit--follow-form-container\" attr-a attr-b attr-c attr-d attr-e attr-f>\n",
    "<form data-v-c76ccf54=\"\" action=\"https://api.follow.it/subscription-form/TUo5R2xpLzYwVVJQeER5Sk5HeXpaUFk2WXlWRXVLUUhNeWgxeVg4V1c4Q0FGVmFHSEttNXlnVkdmTUJCZDBpMlV4MlJwaVpGa2NOc1hOeGdGU2x0eTZVV040Y2pZcTdrMWt1RHRsRzJtRUVhLzg1Nzh2anU5NkF3UE5zZ3RJVlN8K3gzWVA2OGRldnNMVlFHTTIrc2x5MG1tL1ZrTjdEL2xzaktHZ1A3RmlYWT0=/21\" method=\"post\"><div data-v-c76ccf54=\"\" class=\"form-preview\" style=\"background-color: rgb(255, 255, 255); position: relative;\"><div data-v-c76ccf54=\"\" class=\"preview-heading\"><h5 data-v-c76ccf54=\"\" style=\"text-transform: none !important; font-family: Arial; font-weight: bold; color: rgb(0, 0, 0); font-size: 16px; text-align: center;\">Get notification of new posts:</h5></div><div data-v-c76ccf54=\"\" class=\"preview-input-field\"><input data-v-c76ccf54=\"\" type=\"email\" name=\"email\" required=\"\" placeholder=\"Enter your email\" spellcheck=\"false\" style=\"text-transform: none !important; font-family: Arial; font-weight: normal; color: rgb(0, 0, 0); font-size: 14px; text-align: center; background-color: rgb(255, 255, 255);\"></div><div data-v-c76ccf54=\"\" class=\"preview-submit-button\"><button data-v-c76ccf54=\"\" type=\"submit\" style=\"text-transform: none !important; font-family: Arial; font-weight: bold; color: rgb(255, 255, 255); font-size: 16px; text-align: center; background-color: rgb(0, 0, 0);\">Subscribe</button></div></div></form><a href=\"[https://follow.it](https://follow.it/)\" class=\"powered-by-line\">Powered by <img src=\"https://follow.it/static/img/colored-logo.svg\" alt=\"[follow.it](http://follow.it/)\" height=\"17px\"/></a>\n",
    "</div>\n",
    "\n",
    "<div id=\"disqus_thread\"></div>\n",
    "<script>\n",
    "/**\n",
    "*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n",
    "*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */\n",
    "/*\n",
    "var disqus_config = function () {\n",
    "this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable\n",
    "this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n",
    "};\n",
    "*/\n",
    "(function() { // DON'T EDIT BELOW THIS LINE\n",
    "var d = document, s = d.createElement('script');\n",
    "s.src = 'https://https-zahidcseku-github-io-ml-notes.disqus.com/embed.js';\n",
    "s.setAttribute('data-timestamp', +new Date());\n",
    "(d.head || d.body).appendChild(s);\n",
    "})();\n",
    "</script>\n",
    "<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></noscript>\n",
    "\n",
    "<script id=\"dsq-count-scr\" src=\"[//https-zahidcseku-github-io-ml-notes.disqus.com/count.js](notion://https-zahidcseku-github-io-ml-notes.disqus.com/count.js)\" async></script>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "source_map": [
   11,
   253,
   282,
   286,
   317
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}