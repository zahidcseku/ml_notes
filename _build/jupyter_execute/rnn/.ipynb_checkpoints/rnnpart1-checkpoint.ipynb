{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639cd32b",
   "metadata": {},
   "source": [
    "# Disentangling the LSTM model\n",
    "\n",
    "Long Short Term Memory or LSTM is a popular model for sequence data. It is also a source of many confusions\n",
    "for new learner. In this tutorial, I explain the LSTM from various perspective to make it understandable\n",
    "to the new learners.\n",
    "\n",
    "```{note}\n",
    "There are abundance of online materials including videos and blogs to learn LSTM. I have learned a lot\n",
    "from those sources (I listed a few at the end). I encourage everyone to read and learn from as many\n",
    "sources as you can.\n",
    "```\n",
    "\n",
    "Long Short Term Memory or LSTM is a popular neural network model for sequence data. At the same time,\n",
    "it is a source of much confusion for new learners. LSTM belongs to the family of Recurrent Neural\n",
    "Networks (RNN). In this tutorial, I will explain the LSTM from various perspectives to make it more\n",
    "understandable. There are many online materials including videos and blogs to learn LSTM. I have learned\n",
    "much from those sources (I listed a few at the end). I encourage everyone to read and learn from as\n",
    "many sources as possible.\n",
    "\n",
    "I assume that you are convinced that LSTM is good for sequence data and can memorise the\n",
    "order in sequence data. If you are not familiar with this please read [**The Unreasonable Effectiveness\n",
    "of Recurrent Neural Networks**](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [**Understanding\n",
    "LSTM Networks**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). More than $90\\%$ resources on\n",
    "RNN and LSTM include one of the figures (or a variation of them) shown below.\n",
    "\n",
    "```{figure} figure1.png\n",
    "---\n",
    "name: figure1\n",
    "width: 450px\n",
    "align: center\n",
    "---\n",
    "The most common illustrations on RNN and LSTM. (a) Time unrolled RNN model showing how RNNs process\n",
    "inputs sequentially. Remember there is a single network processing the inputs. This image is illustrating the\n",
    "recurrence relations among the inputs. (b) The inner connections in the LSTM model. (c) Showing variations of RNNs\n",
    "based of the number of inputs and outputs. I will not discuss about all the variations in this article. I will\n",
    "only mention in which category our examples belong.\n",
    "\n",
    "Image sources: [**The Unreasonable Effectiveness of Recurrent Neural Networks**]\n",
    "(https://karpathy.github.io/2015/05/21/rnn-effectiveness/),  [**Understanding LSTM Networks**]\n",
    "(https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Computations in feed forward network\n",
    "Before discussing LSTM, let's talk about simple feed forward neural networks (aka multi layer perceptrons or MLP). I like to think of a neural network as computational unit or a parametric function $f_{\\Theta}(.)$ that takes numeric inputs ($\\mathbf X$) of a certain dimension $n_X$ and produces outputs ($\\mathbf y$) of dimension $n_y$ through a series of computations arranged in layers i.e., $\\mathbf y = f_\\Theta(\\mathbf X)$ where $\\Theta$ is the set of parameters i.e., weights and biases. If we have a three layer network then $\\Theta$ includes weights and biases corresponding to the three layers $\\{\\mathbf W^{[1]}, \\mathbf W^{[2]}, \\mathbf W^{[3]}, \\mathbf b^{[1]}, \\mathbf b^{[2]}, \\mathbf b^{[3]}\\}$. (Read [Deep neural network notaion](https://zahidcseku.github.io/ml_notes/notations/notations.html) for a detailed discussion on deep neural network notations and computations).\n",
    "\n",
    "```{note}\n",
    "The input to a neuron unit can be of any dimension (depends on the input features or the dimension of the preceding layer's output), but an individual unit always provides one output. This implies that the number of units in a layer depends on the desired output dimension of the layer. \n",
    "```\n",
    "\n",
    "\n",
    "### Code example\n",
    "\n",
    "Let's generate a dataset $\\mathbf X$ with $n_\\mathcal D$ instances and feed $\\mathbf X$ to a 2-layer MLP with $n^{[1]}=8$ and $n^{[2]}=1$ neuron units ($n^{[l]}$ represents the number of units in layer $l$). From the network, we get $n_y =1$ dimensional outputs $\\mathbf y$ for each instance in $\\mathbf X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13025a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ny' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# create the model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m layer1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(nx, \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m layer2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m8\u001b[39m, \u001b[43mny\u001b[49m)\n\u001b[1;32m     16\u001b[0m net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(layer1, layer2)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# feed the inputs to the network\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ny' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "nd, nx = 15, 5\n",
    "\n",
    "# number of units per layer\n",
    "n1, n2 = 8, 1\n",
    "\n",
    "# generating random inputs\n",
    "X = torch.rand(nd, nx)\n",
    "\n",
    "# create the model\n",
    "layer1 = nn.Linear(nx, 8)\n",
    "layer2 = nn.Linear(8, ny)\n",
    "\n",
    "net = nn.Sequential(layer1, layer2)\n",
    "\n",
    "# feed the inputs to the network\n",
    "y = net(X)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51735d03",
   "metadata": {},
   "source": [
    "Here, `net()` transforms $\\mathbf X \\in \\mathbb R^5$ to produce outputs $\\mathbf y\\in\\mathbb R$. First, `layer1` takes $\\mathbf X$ and produces outputs $\\mathbf a^{[1]}$ (dimension $(n_\\mathcal D, n^{[1]})$). Then, ativations $\\mathbf a^{[1]}$ are fed to `layer2` and layer 2 generates $\\mathbf a^{[2]} = \\mathbf y$ (dimension $(n_\\mathcal D, n_y)$). We consider liner activations in all layers. \n",
    "\n",
    "From [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) we find: `nn.Linear` transforms the data as $y = xA^\\top + b$. We also learn that the shapes of the weights $A$ are `(out_features, in_features)` and biases are `(out_features)`. Hence, for PyTorch, the shapes of weights and biases of layer 1 are `shape($A^{[1]}$)` = $(n^{[1]}, n_X)$ = $(8, 5)$ and `shape($b^{[1]}$)=$8$`. The shapes of weights and biases of layer 2 are `shape($A^{[2]}$)` = $(n^{[2]}, n^{[1]})$ = $(1, 8)$ and `shape($b^{[2]}$)`=$1$. Letâ€™s check the shapes in pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of layer 1 weight matrix: {layer1.weight.size()}\")\n",
    "print(f\"Shape of layer 1 bias vector: {layer1.bias.size()}\")\n",
    "print(f\"Shape of layer 2 weight matrix: {layer2.weight.size()}\")\n",
    "print(f\"Shape of layer 2 bias vector: {layer2.bias.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba99502",
   "metadata": {},
   "source": [
    "We can investigate the outputs of the layers using the equation  $y = xA^\\top + b$. The layer wise computations can be given by:\n",
    "\n",
    "$$\n",
    "a^{[1]} = xA^{[1]\\top} + b^{[1]}\\\\\n",
    "a^{[2]} = xA^{[2]\\top} + b^{[2]}\n",
    "$$\n",
    "\n",
    "For layer 1 outputs we used $a$ to represent activations and for layer 2 the activations $a^{[2]}$ are the outputs $\\mathbf y$. We can implement the equation in PyTorch without passing the inputs through the `net()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d265ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output using pytorch layer\n",
    "a1 = layer1(X)\n",
    "\n",
    "# output using equation\n",
    "# get the weights and biases in layer 1\n",
    "w1 = layer1.weight.data\n",
    "b1 = layer1.bias.data\n",
    "a1_eq = torch.matmul(X, w1.T) + b1\n",
    "\n",
    "print(f\"Layer 1 outputs using pytorch and equation are same: {torch.equal(a1,a1_eq)}\")\n",
    "\n",
    "# output using pytorch layer\n",
    "y = layer2(a1)\n",
    "\n",
    "# output using equation\n",
    "# get the weights and biases in layer 2\n",
    "w2 = layer2.weight.data\n",
    "b2 = layer2.bias.data\n",
    "a2_eq = torch.matmul(a1, w2.T) + b2\n",
    "\n",
    "print(f\"Layer 2 outputs using pytorch and equation are same: {torch.equal(a2,a2_eq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de6564",
   "metadata": {},
   "source": [
    "The network processes $n_\\mathcal D$ instances at a time through matrix multiplication. However, we can achieve the same results using a loop as follows (this simple exercise will help us understand LSTM computations discussed below). \n",
    "\n",
    "```python\n",
    "# initialize a variable same shape as y to store the step wise results\n",
    "loop_y = torch.zeros_like(y)\n",
    "\n",
    "for i in range(X.size()[0]):\n",
    "    loop_y[i] = net(X[i])\n",
    "\n",
    "print(loop_y)\n",
    "```\n",
    "\n",
    "```{note}\n",
    "When implementing neural networks using any framework, such as Pytorch, Keras, TensorFlow, etc., it's important to understand the inputs and outputs specific to that framework. This article will focus on PyTorch.\n",
    "```\n",
    "\n",
    "From PyTorch documentation we find the input and output specification of linear layer `nn.Linear` as input: $(*, H_{in})$ and output: $(*, H_{out})$. $H_{in}$ and $H_{out}$ are the input and output dimensions of a layer. According to the above example task for the first layer $H_{in}=5$ and $H_{out}=8$. The $*$ symbol means that whatever many of $H_{in}$ dimensional instances we feed into the linear layer we get the same number of $H_{out}$ dimensional outputs. The $*$ depends on the batch size (the batch size is the number of instances we want to process at a time). For a batch size of 15 the inputs will be $(15, 5)$ and outputs of the layer will be $(15, 8)$."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "source_map": [
   11,
   69,
   91,
   97,
   102,
   113,
   135
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}