
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deep neural network notation &#8212; Ml Notes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notations/notations';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Disentangling the LSTM model" href="../rnn/rnnpart1.html" />
    <link rel="prev" title="Unlocking the Magic of Machine Learning: A Beginner’s Journey" href="../intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Ml Notes - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Ml Notes - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Unlocking the Magic of Machine Learning: A Beginner’s Journey
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Deep neural network notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rnn/rnnpart1.html">Disentangling the LSTM model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../markdown.html">Markdown Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="../markdown-notebooks.html">Notebooks with MyST Markdown</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/zahidcseku/ml_notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/zahidcseku/ml_notes/issues/new?title=Issue%20on%20page%20%2Fnotations/notations.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notations/notations.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep neural network notation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-of-a-single-neuron">Inputs and outputs of a single neuron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-of-a-single-layer">Inputs and outputs of a single layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-of-a-multiple-layers">Inputs and outputs of a multiple layers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example">Code example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deep-neural-network-notation">
<h1><a class="toc-backref" href="#id2" role="doc-backlink">Deep neural network notation</a><a class="headerlink" href="#deep-neural-network-notation" title="Link to this heading">#</a></h1>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#deep-neural-network-notation" id="id2">Deep neural network notation</a></p>
<ul>
<li><p><a class="reference internal" href="#inputs-and-outputs-of-a-single-neuron" id="id3">Inputs and outputs of a single neuron</a></p></li>
<li><p><a class="reference internal" href="#inputs-and-outputs-of-a-single-layer" id="id4">Inputs and outputs of a single layer</a></p></li>
<li><p><a class="reference internal" href="#inputs-and-outputs-of-a-multiple-layers" id="id5">Inputs and outputs of a multiple layers</a></p></li>
<li><p><a class="reference internal" href="#dataset" id="id6">Dataset</a></p>
<ul>
<li><p><a class="reference internal" href="#example" id="id7">Example</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#code-example" id="id8">Code example</a></p></li>
<li><p><a class="reference internal" href="#summary" id="id9">Summary</a></p></li>
</ul>
</li>
</ul>
</nav>
<p>A deep neural network (DNN) consists of multiple layers of computational units, called neurons. Each layer is arranged sequentially, and the units in each layer are arranged vertically. Each neuron (or unit) takes some input and produces an output. Consequently, a layer takes some inputs and produces some outputs using simple computations. When we say each layer takes inputs, we mean that each neuron in a layer takes inputs. In this section, I will discuss various notations used in deep neural networks. Most of the notations are adapted from Andrew Ng’s deep learning specialization course. The notations involve multiple dimensions that can sometimes be confusing (actually, all the time when you start learning from different sources). When I started learning DNN, I got familiar with one set of notations from Andrew’s course and started representing other materials using the same notations (it was not a trivial task at the beginning but helped me a lot).</p>
<section id="inputs-and-outputs-of-a-single-neuron">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Inputs and outputs of a single neuron</a><a class="headerlink" href="#inputs-and-outputs-of-a-single-neuron" title="Link to this heading">#</a></h2>
<p>Each neuron takes arbitrary <span class="math notranslate nohighlight">\(n_X\)</span> dimensional inputs <span class="math notranslate nohighlight">\(\mathbf X\)</span> and produces a single output <span class="math notranslate nohighlight">\(a\)</span>.</p>
<figure class="align-center" id="single-neuron">
<a class="reference internal image-reference" href="../_images/single-neuron.png"><img alt="../_images/single-neuron.png" src="../_images/single-neuron.png" style="width: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">A single neural processing unit which takes <span class="math notranslate nohighlight">\(n_X\)</span> dimensional inputs and generates a scalar output <span class="math notranslate nohighlight">\(a\)</span>.</span><a class="headerlink" href="#single-neuron" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The neuron’s output is calculated in two steps, as indicated by a circular + symbol and a rectangular <span class="math notranslate nohighlight">\(\sigma\)</span> symbol. A weighted sum is first computed as follows:</p>
<div class="math notranslate nohighlight">
\[z = x_1*w_1 + x_2*w_2 + \cdots + x_{n_X}*w_{n_X} + b = \sum_{i=1}^{n_X}x_i*w_i +b\]</div>
<p>And then an activation function <span class="math notranslate nohighlight">\(\sigma\)</span> is applied to <span class="math notranslate nohighlight">\(z\)</span> to compute the output (called activation) of the unit as follows:</p>
<div class="math notranslate nohighlight">
\[a = \sigma\left(z\right)\]</div>
<p>If we express the input and the weights as row vectors <span class="math notranslate nohighlight">\(\mathbf W = [w_1, w_2, \cdots, w_{n_X}]\)</span> and <span class="math notranslate nohighlight">\(\mathbf X  =[x_1, x_2,\dots,x_{n_X}]\)</span>, we can achieve the same result using a vector operation:</p>
<div class="math notranslate nohighlight">
\[a = \sigma(z) = \sigma(\mathbf W \cdot \mathbf X^\top + b)\]</div>
<p><strong>Shapes</strong>: both the <span class="math notranslate nohighlight">\(\mathbf W\)</span> and <span class="math notranslate nohighlight">\(\mathbf X\)</span> are row vectors of shape <span class="math notranslate nohighlight">\((1, n_X)\)</span> and both <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(a\)</span> are scalars.</p>
<div class="pst-scrollable-table-container"><table class="table" id="notation-single-neuron">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Notations - single neuron</span><a class="headerlink" href="#notation-single-neuron" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Shape</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf X\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((1, n_X)\)</span></p></td>
<td><p>The input row vector.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(n_X\)</span></p></td>
<td><p>1</p></td>
<td><p>The dimension of the inputs.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_j\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p>The value of the <span class="math notranslate nohighlight">\(j\)</span>-th feature of an input object.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf W\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((1, n_X)\)</span></p></td>
<td><p>Weight vector of the neuron.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(b\)</span></p></td>
<td><p>1</p></td>
<td><p>Bias parameter of the neuron.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(z\)</span></p></td>
<td><p>1</p></td>
<td><p>The intermediate output of the neuron before applying the activation function.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(a\)</span></p></td>
<td><p>1</p></td>
<td><p>The output (or activation) of the neuron after applying the activation function <span class="math notranslate nohighlight">\(\sigma\)</span> to <span class="math notranslate nohighlight">\(z\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf X^\top\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n_X, 1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf X\)</span> transposed.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="inputs-and-outputs-of-a-single-layer">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Inputs and outputs of a single layer</a><a class="headerlink" href="#inputs-and-outputs-of-a-single-layer" title="Link to this heading">#</a></h2>
<p>A layer in a neural network is created by stacking multiple neural units. Each unit takes the same number of real numbers (<span class="math notranslate nohighlight">\(n_X\)</span>) and produces a single output. The number of outputs of a layer is determined by the number of units in the layer. Representing a layer by <span class="math notranslate nohighlight">\(l\)</span>, the number of units in the layer is expressed as <span class="math notranslate nohighlight">\(n^{[l]}\)</span>.</p>
<figure class="align-center" id="single-layer">
<a class="reference internal image-reference" href="../_images/single_layer.png"><img alt="../_images/single_layer.png" src="../_images/single_layer.png" style="width: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">A single layer processing unit that takes <span class="math notranslate nohighlight">\(n_X\)</span> dimensional inputs and produces <span class="math notranslate nohighlight">\(n^{[l]}\)</span> dimensional outputs. Here, <span class="math notranslate nohighlight">\(n^{[l]}\)</span> is the number of processing units in layer <span class="math notranslate nohighlight">\(l\)</span>.</span><a class="headerlink" href="#single-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Now that we have multiple units, we can’t express the weights as a vector anymore. The weights of each individual unit in the layer are still a vector (shape <span class="math notranslate nohighlight">\((1, n_X)\)</span>), and we need a matrix notation (by stacking the vectors) to express the weights of layer <span class="math notranslate nohighlight">\(l\)</span>. For <span class="math notranslate nohighlight">\(n^{[l]}\)</span> units, we will have <span class="math notranslate nohighlight">\(n^{[l]}\)</span> biases expressed as <span class="math notranslate nohighlight">\(\mathbf b^{[l]}\)</span>, which is a vector of shape <span class="math notranslate nohighlight">\((n^{[l]}, 1)\)</span>. We have multiple outputs from a layer, hence both <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(z\)</span> are now vectors of shape <span class="math notranslate nohighlight">\((n^{[l]}, 1)\)</span> and represented by: <span class="math notranslate nohighlight">\(\mathbf z^{[l]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf a^{[l]}\)</span> respectively. Layer <span class="math notranslate nohighlight">\(l\)</span> performs the following operations:</p>
<div class="math notranslate nohighlight">
\[
a^{[l]}_1 = \sigma^{[l]} (z^{[l]}_1) = \sigma^{[l]} \left(w^{[l]}_{11}x_1 + w^{[l]}_{12}x_2+\dots+w^{[l]}_{1n_X}x_{n_X}+b^{[l]}_1\right) = \sigma^{[l]} \left(\sum_i^{n_X} w^{[l]}_{1i}x^i + b^{[l]}_1\right)
\]</div>
<div class="math notranslate nohighlight">
\[
a^{[l]}_2 = \sigma^{[l]} (z^{[l]}_2) = \sigma^{[l]} \left(w^{[l]}_{21}x_1 + w^{[l]}_{22}x_2+\dots+w^{[l]}_{2n_X}x_{n_X}+b^{[l]}_2\right)=\sigma^{[l]}\left(\sum_{i}^{n_X} w^{[l]}_{2i}x_i + b^{[l]}_2\right)
\]</div>
<div class="math notranslate nohighlight">
\[\dots\]</div>
<div class="math notranslate nohighlight">
\[a^{[l]}_{n^{[l]}} = \sigma^{[l]} (z^{[l]}_{n^{[l]}}) = \sigma^{[l]} \left(w^{[l]}_{n^{[l]}1}x_1 + w^{[l]}_{n^{[l]}2}x_2+\dots+w^{[l]}_{n^{[l]}n_X}x_{n_X}+b^{[l]}_{n^{[l]}}\right)=\sigma^{[l]}\left(\sum_{i}^{n_X} w^{[l]}_{n^{[l]}i}x_i + b^{[l]}_{n^{[l]}}\right)\]</div>
<p>We can implement the above computations in python using only multiplications and additions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nl</span><span class="p">):</span>
   <span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
   <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
      <span class="n">z</span> <span class="o">+=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
   <span class="n">z</span> <span class="o">+=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
   <span class="n">a</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
   
   <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<p>The equations and computations can be simplified using matrix operations. The weights of the matrix can be represented by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf W^{[l]} = \begin{bmatrix} w^{[l]}_{11} &amp; w^{[l]}_{12} &amp; \cdots &amp; w^{[l]}_{1n_X}\\ w^{[l]}_{21} &amp; w^{[l]}_{22} &amp; \cdots &amp; w^{[l]}_{2n_X}\\\cdots&amp;\cdots&amp;\cdots&amp;\cdots\\w^{[l]}_{n^{[l]}1} &amp; w^{[l]}_{n^{[l]}2} &amp; \cdots &amp; w^{[l]}_{n^{[l]}n_X}\end{bmatrix}
\end{split}\]</div>
<p>The dimension of <span class="math notranslate nohighlight">\(\mathbf W^{[l]}\)</span> depends on the input dimension <span class="math notranslate nohighlight">\(n_X\)</span> and the number of units (which is same as the output dimension of the layer) <span class="math notranslate nohighlight">\(n^{[l]}\)</span> i.e., <span class="math notranslate nohighlight">\((n^{[l]}, n_X)\)</span>. Using the matrix notations, the computations are:</p>
<div class="math notranslate nohighlight">
\[\mathbf a^{[l]} = \sigma^{[l]} (\mathbf z^{[l]}) = \sigma^{[l]}\left(\mathbf W^{[l]} \cdot \mathbf X^\top + \mathbf b^{[l]}\right)\]</div>
<p>In python implementation,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="notation-single-layer">
<caption><span class="caption-number">Table 2 </span><span class="caption-text">Notations - single layer of neurons</span><a class="headerlink" href="#notation-single-layer" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Shape</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(l\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p>The layer id.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(n^{[l]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p>Number of units in layer <span class="math notranslate nohighlight">\(l\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf b^{[l]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[l]}, 1)\)</span></p></td>
<td><p>The bias vector corresponding to layer <span class="math notranslate nohighlight">\(l\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(b^{[l]}_i\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p>Bias of the <span class="math notranslate nohighlight">\(i\)</span>-th unit in layer <span class="math notranslate nohighlight">\(l\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf W^{[l]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[l]}, n_X)\)</span></p></td>
<td><p>The weights corresponding to the layer <span class="math notranslate nohighlight">\(l\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(w^{[l]}_{ij}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p>The weight corresponding to unit <span class="math notranslate nohighlight">\(i\)</span> and feature <span class="math notranslate nohighlight">\(j\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf a^{[l]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[l]}, 1)\)</span></p></td>
<td><p>Activations (outputs) of layer <span class="math notranslate nohighlight">\(l\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf z^{[l]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[l]}, 1)\)</span></p></td>
<td><p>The intermediate outputs of the neurons given the input  <span class="math notranslate nohighlight">\(\mathbf X\)</span> before applying the activation function.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(a_i^{[l]}, z_i^{[l]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p>Elements of <span class="math notranslate nohighlight">\(\mathbf a^{[l]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf z^{[l]}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma^{[l]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(~~\)</span></p></td>
<td><p>Activation function corresponding to layer <span class="math notranslate nohighlight">\(l\)</span>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="inputs-and-outputs-of-a-multiple-layers">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Inputs and outputs of a multiple layers</a><a class="headerlink" href="#inputs-and-outputs-of-a-multiple-layers" title="Link to this heading">#</a></h2>
<p>A single neuron unit or a single layer network is never used for solving practical machine learning problems. Building upon the discussion of a single neuron unit and a single layer of neurons, we now discuss the practical scenario where multiple layers of neurons are arranged sequentially as shown in the figure below. The input <span class="math notranslate nohighlight">\(\mathbf X \in \mathbb R^{n_X}\)</span> is fed to the model at layer <span class="math notranslate nohighlight">\(l=1\)</span> which processes the input and produces an output vector <span class="math notranslate nohighlight">\(\mathbf a^{[1]} \in \mathbb R^{n^{[1]}}\)</span>. The layer <span class="math notranslate nohighlight">\(2\)</span> receives inputs <span class="math notranslate nohighlight">\(\mathbf a^{n^{[1]}} \in \mathbb R^{n^{[1]}}\)</span> from layer <span class="math notranslate nohighlight">\(1\)</span> and produces outputs <span class="math notranslate nohighlight">\(\mathbf a^{[2]} \in \mathbb R^{n^{[2]}}\)</span> and so on. Remember that each neuron receives the same dimensional inputs and the output dimension of a layer is equal to the number of units. Each layer (except the first layer) receives input from the previous layer. The number of units in a layer is a design choice (except the last layer <span class="math notranslate nohighlight">\(L\)</span>) determined through hyperparameter search. The last layer from which we obtain the outputs of the model is called the output layer. The input dimension of the first layer and the number of units of layer <span class="math notranslate nohighlight">\(L\)</span> (or the output dimension of layer <span class="math notranslate nohighlight">\(L\)</span>) depends on the task. For example, for the digit classification of <span class="math notranslate nohighlight">\(28\times 28\)</span> images of digits, the input dimension <span class="math notranslate nohighlight">\(n_X=28*28=784\)</span> and the output dimension or the number of units in layer <span class="math notranslate nohighlight">\(L\)</span> is <span class="math notranslate nohighlight">\(10\)</span>.</p>
<figure class="align-center" id="multi-layer-model">
<a class="reference internal image-reference" href="../_images/multi_layer_model.png"><img alt="../_images/multi_layer_model.png" src="../_images/multi_layer_model.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">A multilayer deep neural network (or a multilayer perceptron) model. Each layer <span class="math notranslate nohighlight">\(l\)</span> consists of <span class="math notranslate nohighlight">\(n^{[l]}\)</span> nuerons.</span><a class="headerlink" href="#multi-layer-model" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>All the symbols used in the above <a class="reference internal" href="#multi-layer-model"><span class="std std-ref">figure</span></a> are covered in the previous section: Inputs and Outputs of a Single Layer. Often, we use <span class="math notranslate nohighlight">\(\mathbf {\hat y}\)</span> to represent the output of the model. So, we can say <span class="math notranslate nohighlight">\(\mathbf{\hat y}=\mathbf a^{[L]}\)</span>. In addition to our previous symbols, we introduce the following symbol for multilayer models.</p>
<div class="pst-scrollable-table-container"><table class="table" id="notation-multiple-layers">
<caption><span class="caption-number">Table 3 </span><span class="caption-text">Notations - multiple layers of neurons</span><a class="headerlink" href="#notation-multiple-layers" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Shape</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(L\)</span></p></td>
<td><p>1</p></td>
<td><p>The number of layers.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(1,2,\dots,l,\dots,L\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p>Layer id.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf{\hat y} = \mathbf a^{[L]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((a^{[L]}, 1)\)</span></p></td>
<td><p>Output of the model.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf X = \mathbf a^{[0]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((1, n_X)\)</span></p></td>
<td><p>Inputs in terms of activation.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Using the activation notations, <span class="math notranslate nohighlight">\(\mathbf a^{[0]}=\mathbf X^\top\)</span> and <span class="math notranslate nohighlight">\(\mathbf a^{[L]}=\mathbf{\hat y}\)</span> we can draw as general eqaution ov the network as:</p>
<div class="math notranslate nohighlight">
\[\mathbf z^{[l]} = W^{[l]}\cdot \mathbf a^{[l-1]} + \mathbf b^{[l]}\]</div>
<div class="math notranslate nohighlight">
\[\mathbf a^{[l]} = \sigma^{[l]}\left (\mathbf z^{[l]}\right)\]</div>
<p>for <span class="math notranslate nohighlight">\(l=1,2,\dots,L\)</span></p>
</section>
<section id="dataset">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Dataset</a><a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<p>In the above discussion, we considered our input <span class="math notranslate nohighlight">\(\mathbf X\in\mathbb R^{n_X}\)</span> to be a <span class="math notranslate nohighlight">\(n_X\)</span> dimensional row vector. However, in real applications, the input is a <span class="math notranslate nohighlight">\(2\)</span> or <span class="math notranslate nohighlight">\(3\)</span> dimensional tensor (we consider 2d cases only for now). For supervised learning, we have many instances of <span class="math notranslate nohighlight">\(\mathbf X\)</span> and their associated labels <span class="math notranslate nohighlight">\(\mathbf y\)</span>. For unsupervised learning tasks, we do not have <span class="math notranslate nohighlight">\(\mathbf y\)</span>. We represent the dataset as <span class="math notranslate nohighlight">\(\mathcal D=(\mathbf X, \mathbf y)\)</span>. The shape of <span class="math notranslate nohighlight">\(\mathbf X\)</span> is given by <span class="math notranslate nohighlight">\((n_{\mathcal D}, n_X)\)</span> and the shape of <span class="math notranslate nohighlight">\(\mathbf y\)</span> is <span class="math notranslate nohighlight">\((n_\mathcal D, n_y)\)</span>. To refer to any instance of <span class="math notranslate nohighlight">\(\mathcal D\)</span>, we use the superscript notation i.e., <span class="math notranslate nohighlight">\(\mathbf X^{(i)}\)</span> refers to the <span class="math notranslate nohighlight">\(i\)</span>-th instance of <span class="math notranslate nohighlight">\(\mathbf X\)</span> and <span class="math notranslate nohighlight">\(\mathbf y^{(i)}\)</span> is the corresponding label. The superscript notation can also be used to refer to any layer computation, e.g., <span class="math notranslate nohighlight">\(\mathbf z^{(i)[l]}\)</span> refers to the output of layer <span class="math notranslate nohighlight">\(l\)</span> corresponding to the <span class="math notranslate nohighlight">\(i\)</span> instance in the dataset.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>we used the same <span class="math notranslate nohighlight">\(\mathbf X\)</span> to refer to the input as a vector above for simplicity. We should have used <span class="math notranslate nohighlight">\(\mathbf X^{(i)}\)</span>. With this dataset notation in the above computations, all output shapes will change as follows: <span class="math notranslate nohighlight">\(\mathbf z^{[l]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf a^{[l]}\)</span> now include <span class="math notranslate nohighlight">\(n_{\mathcal D}\)</span> instances corresponding to the number of samples in the dataset.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For efficiency, the dataset is divided into batches of inputs and the network processes a batch at a time rather than the entire batch at a time.</p>
</div>
<section id="example">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Example</a><a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>The <a class="reference external" href="https://archive.ics.uci.edu/dataset/53/iris">iris dataset:</a> includes 150 instances of iris flower. Each isntance is described by four feautures {sepal length, sepal width, petal length, petal width}. In our notation,  the iris dataset can be described as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n_{\mathcal D} =150\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n_X=4\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n_y=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf X^{(2)}= [4.9, 3.0, 1.4, 0.2]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf y^{(2)} = [C1]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x^{(150)}_3=5.1\)</span></p></li>
</ul>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/dataset.png"><img alt="../_images/dataset.png" src="../_images/dataset.png" style="width: 250px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">The iris dataset.</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="code-example">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Code example</a><a class="headerlink" href="#code-example" title="Link to this heading">#</a></h2>
<p>In this code example, we will generate 10 random instances of 5 dimensional inputs i.e., <span class="math notranslate nohighlight">\(n_\mathcal D=10\)</span>, <span class="math notranslate nohighlight">\(n_X=5\)</span>. Let’s consider the network consists of three layers (<span class="math notranslate nohighlight">\(L=3\)</span>) with <span class="math notranslate nohighlight">\(n^{[1]} = 8\)</span>, <span class="math notranslate nohighlight">\(n^{[2]} = 12\)</span>, and <span class="math notranslate nohighlight">\(n^{[3]} = 2\)</span>. The dimensions of our weight metrices will be <span class="math notranslate nohighlight">\(\mathbf W^{[1]}\)</span>, <span class="math notranslate nohighlight">\(\mathbf W^{[2]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf W^{[3]}\)</span> are <span class="math notranslate nohighlight">\((8, 5)\)</span>, <span class="math notranslate nohighlight">\((12, 8)\)</span> and <span class="math notranslate nohighlight">\((2, 12)\)</span>. The shapes of the biases <span class="math notranslate nohighlight">\(\mathbf b^{[1]}\)</span>, <span class="math notranslate nohighlight">\(\mathbf b^{[2]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf b^{[3]}\)</span> are <span class="math notranslate nohighlight">\((8, 1)\)</span>, <span class="math notranslate nohighlight">\((12, 1)\)</span> and <span class="math notranslate nohighlight">\((2, 1)\)</span>. We will generate the parameters randomly and using the equations discussed above we will compute <span class="math notranslate nohighlight">\(\mathbf a^{[l]}, l = 1,2,3\)</span>. We assume <span class="math notranslate nohighlight">\(\mathbf a^{[l]} = \mathbf z^{[l]} | l =1,2,3\)</span> i.e., we do not apply the activation functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set up the variables</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">nd</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">nl</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">n1</span><span class="p">,</span> <span class="n">n2</span><span class="p">,</span> <span class="n">n3</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span>

<span class="c1"># input and parameters initializations</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nd</span><span class="p">,</span> <span class="n">nx</span><span class="p">)</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n1</span><span class="p">,</span> <span class="n">nx</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n2</span><span class="p">,</span> <span class="n">n1</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">W3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n3</span><span class="p">,</span> <span class="n">n2</span><span class="p">)</span>
<span class="n">b3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># layer wise computations</span>
<span class="n">a0</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
<span class="n">a3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Outputs of the network&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Outputs of the network
[[39.06086285 43.72346756 37.12474203 38.53765622 50.31768436 43.89814926
  41.52777479 48.1064023  24.98654149 35.02019868]
 [35.69983835 39.85387931 34.27788184 35.26641974 46.21650293 40.45132509
  37.80900998 44.12492699 22.45267836 32.06402327]]
</pre></div>
</div>
</div>
</div>
<p>Let’s, verify the results using pytorch. We will initialize the pytorch parameters and inputs using the same random tensors as our previous example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span> 

<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">n1</span><span class="p">)</span>
<span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n1</span><span class="p">,</span> <span class="n">n2</span><span class="p">)</span>
<span class="n">l3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n2</span><span class="p">,</span> <span class="n">n3</span><span class="p">)</span>

<span class="n">tensorX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># initialize same weights</span>
<span class="n">l1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
<span class="n">l1</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

<span class="n">l2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>
<span class="n">l2</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">b2</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

<span class="n">l3</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">W3</span><span class="p">)</span>
<span class="n">l3</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">b3</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

<span class="n">a1</span> <span class="o">=</span> <span class="n">l1</span><span class="p">(</span><span class="n">tensorX</span><span class="p">)</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
<span class="n">a3</span> <span class="o">=</span> <span class="n">l3</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[39.0609, 35.6998],
        [43.7235, 39.8539],
        [37.1247, 34.2779],
        [38.5377, 35.2664],
        [50.3177, 46.2165],
        [43.8981, 40.4513],
        [41.5278, 37.8090],
        [48.1064, 44.1249],
        [24.9865, 22.4527],
        [35.0202, 32.0640]], dtype=torch.float64, grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>pytorch uses broadcasting in computations. Here, we used <code class="docutils literal notranslate"><span class="pre">squeeze()</span></code> to match the shape of the bias parameter with pytorch internals.</p>
</div>
<p>We see that outputs of both implementations are identical. That’s what we expect!</p>
</section>
<section id="summary">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Summary</a><a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>An entire multilayer neural network can be expressed as a set of parameters - the weights and biases.</p>
<p><span class="math notranslate nohighlight">\(\mathbf{\hat y} = f_\Theta(\mathbf X)\)</span> where <span class="math notranslate nohighlight">\(\Theta = \{\mathbf W^{[l]}, \mathbf b^{[l]}~|~l = 1,2, \dots, L\}\)</span>.</p>
<p>Let’s consider the number of units of a 3 layer network to be <span class="math notranslate nohighlight">\(n^{[1]}=8, n^{[2]}=16, n^{[3]}=3\)</span> and the input dimension <span class="math notranslate nohighlight">\((n_\mathcal D, n_X) = (500, 4)\)</span>. In layer 1, we have 8 units i.e., the output dimensions of layer 1 (also the input dimension of layer 2) is 8, <span class="math notranslate nohighlight">\(\mathbf W^{[1]}\)</span> will have 8 rows (weights of each unit is a row in the weight matrix) and each unit of layer 1 will process 4 dimensional inputs (as <span class="math notranslate nohighlight">\(n_X=4\)</span>) that means each unit will require 4 weights i.e, <span class="math notranslate nohighlight">\(\mathbf W^{[1]}\)</span> has 4 columns. The shape of <span class="math notranslate nohighlight">\(\mathbf W^{[1]}\)</span> is <span class="math notranslate nohighlight">\((8, 4)\)</span>. In our notation, the input <span class="math notranslate nohighlight">\(\mathbf X^\top = \mathbf a^{[0]}\)</span>, the output of layer 1 is <span class="math notranslate nohighlight">\(\mathbf a^{[1]}\)</span> which is <span class="math notranslate nohighlight">\(8\)</span> dimensional that implies each unit of layer 2 will process <span class="math notranslate nohighlight">\(8\)</span> dimensional inputs i.e., <span class="math notranslate nohighlight">\(\mathbf W^{[2]}\)</span> will have <span class="math notranslate nohighlight">\(8\)</span> columns and with <span class="math notranslate nohighlight">\(n^{[2]}=16\)</span> units the number of rows in <span class="math notranslate nohighlight">\(\mathbf W^{[2]}\)</span> is <span class="math notranslate nohighlight">\(16\)</span>. Similarly, the shape of <span class="math notranslate nohighlight">\(\mathbf W^{[3]}\)</span> is <span class="math notranslate nohighlight">\((3, 16)\)</span>.</p>
<p>The dimensions of <span class="math notranslate nohighlight">\(\mathbf W\)</span> and <span class="math notranslate nohighlight">\(\mathbf b\)</span> are:</p>
<ul class="simple">
<li><p>For, <span class="math notranslate nohighlight">\(l=1\)</span>, shape of <span class="math notranslate nohighlight">\(\mathbf W^{[1]}=(8,4)\)</span> and shape of <span class="math notranslate nohighlight">\(\mathbf b^{[1]}=(8,1)\)</span>.</p></li>
<li><p>For, <span class="math notranslate nohighlight">\(l=2\)</span>, shape of <span class="math notranslate nohighlight">\(\mathbf W^{[2]}=(16,8)\)</span> and shape of <span class="math notranslate nohighlight">\(\mathbf b^{[2]}=(16,1)\)</span>.</p></li>
<li><p>For, <span class="math notranslate nohighlight">\(l=3\)</span>, shape of <span class="math notranslate nohighlight">\(\mathbf W^{[3]}=(3,16)\)</span> and shape of <span class="math notranslate nohighlight">\(\mathbf b^{[3]}=(3,1)\)</span>.</p></li>
</ul>
<p>Computations in the network are:</p>
<div class="math notranslate nohighlight">
\[\mathbf a^{[1]} = \sigma^{[1]}\left(W^{[1]}\cdot \mathbf a^{[0]} + \mathbf b^{[1]}\right)\]</div>
<div class="math notranslate nohighlight">
\[\mathbf a^{[2]} = \sigma^{[2]}\left(W^{[2]}\cdot \mathbf a^{[1]} + \mathbf b^{[2]}\right)\]</div>
<div class="math notranslate nohighlight">
\[\mathbf a^{[3]} = \sigma^{[3]}\left(W^{[3]}\cdot \mathbf a^{[2]} + \mathbf b^{[3]}\right)=\mathbf{\hat y}\]</div>
<p>The shapes of <span class="math notranslate nohighlight">\(\mathbf a^{[1]}, \mathbf a^{[2]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf a^{[3]}\)</span>  are <span class="math notranslate nohighlight">\((8, 500), (16, 500)\)</span> and <span class="math notranslate nohighlight">\((3, 500)\)</span>.</p>
<p>In future, I will discuss the forward and backward propagation through a neural network using a complete example. Stay tuned:</p>
<style>
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview {
display: flex !important;
flex-direction: column !important;
justify-content: center !important;
margin-top: 30px !important;
padding: clamp(17px, 5%, 40px) clamp(17px, 7%, 50px) !important;
max-width: none !important;
border-radius: 6px !important;
box-shadow: 0 5px 25px rgba(34, 60, 47, 0.25) !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview,
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview *{
box-sizing: border-box !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-heading {
width: 100% !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-heading h5{
margin-top: 0 !important;
margin-bottom: 0 !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field {
margin-top: 20px !important;
width: 100% !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input {
width: 100% !important;
height: 40px !important;
border-radius: 6px !important;
border: 2px solid #e9e8e8 !important;
background-color: #fff;
outline: none !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input {
color: #000000 !important;
font-family: "Montserrat" !important;
font-size: 14px;
font-weight: 400;
line-height: 20px;
text-align: center;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input::placeholder {
color: #000000 !important;
opacity: 1 !important;
}

.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input:-ms-input-placeholder {
color: #000000 !important;
}

.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-input-field input::-ms-input-placeholder {
color: #000000 !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-submit-button {
margin-top: 10px !important;
width: 100% !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-submit-button button {
width: 100% !important;
height: 40px !important;
border: 0 !important;
border-radius: 6px !important;
line-height: 0px !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .form-preview .preview-submit-button button:hover {
cursor: pointer !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .powered-by-line {
color: #231f20 !important;
font-family: "Montserrat" !important;
font-size: 13px !important;
font-weight: 400 !important;
line-height: 25px !important;
text-align: center !important;
text-decoration: none !important;
display: flex !important;
width: 100% !important;
justify-content: center !important;
align-items: center !important;
margin-top: 10px !important;
}
.followit--follow-form-container[attr-a][attr-b][attr-c][attr-d][attr-e][attr-f] .powered-by-line img {
margin-left: 10px !important;
height: 1.13em !important;
max-height: 1.13em !important;
}

</style>
<div class="followit--follow-form-container" attr-a attr-b attr-c attr-d attr-e attr-f>
<form data-v-c76ccf54="" action="https://api.follow.it/subscription-form/TUo5R2xpLzYwVVJQeER5Sk5HeXpaUFk2WXlWRXVLUUhNeWgxeVg4V1c4Q0FGVmFHSEttNXlnVkdmTUJCZDBpMlV4MlJwaVpGa2NOc1hOeGdGU2x0eTZVV040Y2pZcTdrMWt1RHRsRzJtRUVhLzg1Nzh2anU5NkF3UE5zZ3RJVlN8K3gzWVA2OGRldnNMVlFHTTIrc2x5MG1tL1ZrTjdEL2xzaktHZ1A3RmlYWT0=/21" method="post"><div data-v-c76ccf54="" class="form-preview" style="background-color: rgb(255, 255, 255); position: relative;"><div data-v-c76ccf54="" class="preview-heading"><h5 data-v-c76ccf54="" style="text-transform: none !important; font-family: Arial; font-weight: bold; color: rgb(0, 0, 0); font-size: 16px; text-align: center;">Get notification of new posts:</h5></div><div data-v-c76ccf54="" class="preview-input-field"><input data-v-c76ccf54="" type="email" name="email" required="" placeholder="Enter your email" spellcheck="false" style="text-transform: none !important; font-family: Arial; font-weight: normal; color: rgb(0, 0, 0); font-size: 14px; text-align: center; background-color: rgb(255, 255, 255);"></div><div data-v-c76ccf54="" class="preview-submit-button"><button data-v-c76ccf54="" type="submit" style="text-transform: none !important; font-family: Arial; font-weight: bold; color: rgb(255, 255, 255); font-size: 16px; text-align: center; background-color: rgb(0, 0, 0);">Subscribe</button></div></div></form><a href="[https://follow.it](https://follow.it/)" class="powered-by-line">Powered by <img src="https://follow.it/static/img/colored-logo.svg" alt="[follow.it](http://follow.it/)" height="17px"/></a>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-zahidcseku-github-io-ml-notes.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script id="dsq-count-scr" src="[//https-zahidcseku-github-io-ml-notes.disqus.com/count.js](notion://https-zahidcseku-github-io-ml-notes.disqus.com/count.js)" async></script></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notations"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Unlocking the Magic of Machine Learning: A Beginner’s Journey</p>
      </div>
    </a>
    <a class="right-next"
       href="../rnn/rnnpart1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Disentangling the LSTM model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-of-a-single-neuron">Inputs and outputs of a single neuron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-of-a-single-layer">Inputs and outputs of a single layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-of-a-multiple-layers">Inputs and outputs of a multiple layers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example">Code example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Zahid Islam
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>